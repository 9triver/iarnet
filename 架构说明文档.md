# IARNet 系统架构与部署流程综述

IARNet（Intelligent Application Runtime Network）是一个面向算力网络的智能化应用开发运行平台，采用分层分布式架构设计，实现了从用户应用导入到跨节点资源调度的完整应用生命周期管理。系统整体架构自上而下划分为三个核心层次：用户交互层、应用管理层和算力执行层。用户交互层包含前端页面组件，为用户提供应用管理、资源监控和任务执行等交互功能，通过 RESTful API 与后端服务进行交互。应用管理层包含应用管理器、执行引擎两个核心组件，分别负责应用生命周期管理和任务执行调度。算力执行层包含域资源管理器、资源提供者节点、本地资源管理器、Actor 执行单元和数据总线等组件，域资源管理器作为算力层提供给应用层的算力网络入口，负责资源分配调度和实际计算资源的提供。三个层次通过模块化设计和消息驱动机制协同工作，实现了多节点算力资源的统一调度与协同执行。

在用户交互层，前端页面组件作为系统与用户交互的唯一入口，用户通过浏览器访问系统功能界面，提交应用导入请求时将应用元数据、Git 仓库地址、运行环境等信息封装成 HTTP 请求发送给应用管理器。

在应用管理层，应用管理器负责应用的全生命周期操作，接收应用创建请求后创建应用元数据并异步执行 Git 仓库克隆，克隆完成后创建运行器容器启动应用主程序。运行器容器内运行的应用主程序基于课题一提出的组件化开发框架构建，该框架为应用提供了函数定义、工作流编排和任务调度等核心能力。应用主程序启动后连接到课题二提出的执行引擎，建立控制会话。执行引擎基于课题二提出的核心技术实现，为每个应用创建控制器实例，维护函数定义、有向无环图（DAG）和 Actor 组信息，采用依赖驱动的调度策略根据 DAG 节点依赖关系触发任务执行。当应用需要执行计算任务时，执行引擎分析任务的资源需求，包括 CPU、内存、GPU 等计算资源以及资源标签要求，将资源需求封装成资源请求发送给算力执行层的域资源管理器。

在算力执行层，域资源管理器是课题三提出的资源调度系统的核心组件，作为算力层提供给应用层的算力网络入口，集成了 Store 服务、组件管理、资源提供者管理等子模块。域资源管理器首先在本地节点查找可用的资源提供者，如果本地资源不足则通过发现服务查找其他 IARNet 节点（对等节点），发现服务基于 Gossip 协议实现多跳消息传播，在节点间交换资源状态信息。若仍无法满足资源需求，域资源管理器将调度请求委托给全局调度器进行跨节点调度。资源提供者节点（Provider）是课题三提出的资源调度系统的重要组成部分，每个资源提供者节点封装或集成一个本地资源管理器，本地资源管理器负责与底层资源平台交互，执行容器的创建、启动、停止和删除等生命周期管理操作。不同资源提供者节点采用不同的本地资源管理器实现：Docker Provider 以 Docker 引擎作为其本地资源管理器，Kubernetes Provider 以 Kubernetes 集群作为其本地资源管理器，Process Provider 和 OpenStack Provider 则分别采用进程管理和 OpenStack 云平台作为其本地资源管理器。这种设计架构使得系统能够流畅适配当前主流的资源管理技术，通过统一的资源提供者接口对 IARNet 平台屏蔽了底层资源管理技术的实现差异，实现了良好的可扩展性，便于未来接入更多类型的资源管理平台。当域资源管理器确定目标资源提供者后，调用资源提供者的部署接口创建容器实例，容器内部运行组件程序。组件是系统的核心执行单元，每个组件封装一个 Actor，Actor 是实际执行计算任务的实体，采用消息驱动的异步处理模式。组件启动后通过 gRPC 连接到执行引擎，建立执行会话，接收函数定义消息并注册到本地运行时环境，注册完成后通过数据总线向上游发送就绪消息。数据总线是系统内组件间通信的核心基础设施，采用 Router-Dealer 模式的 ZMQ 通道实现消息路由，执行引擎通过数据总线向 Actor 发送函数调用请求，Actor 从 Store 服务获取参数、执行计算、保存结果，并通过数据总线返回执行响应。Store 服务作为域资源管理器的重要组成部分，存储函数参数和结果，实现了数据的解耦，为 Actor 之间的数据共享和传递提供了统一的数据存储接口。

系统各组件之间的交互遵循严格的消息传递流程：用户通过前端页面组件提交应用导入请求，应用管理器创建工作空间并执行 Git 克隆，克隆完成后创建运行器容器启动应用主程序，应用主程序基于课题一提出的组件化开发框架构建，连接到课题二提出的执行引擎建立控制会话。当应用需要执行计算任务时，执行引擎分析资源需求并向域资源管理器发送资源请求，域资源管理器在本地节点或跨节点范围内查找可用资源，确定目标资源提供者后调用部署接口创建组件容器。组件容器内运行的 Actor 通过数据总线与执行引擎通信，接收函数定义和调用请求，执行计算任务并返回结果。执行引擎根据 DAG 依赖关系调度任务执行，通过数据总线协调多个 Actor 协同工作，完成复杂工作流的执行。整个系统通过分层设计、模块化解耦和消息驱动机制，实现了多节点算力资源的统一管理和协同执行，支持多种资源提供者类型的灵活接入，通过跨节点调度机制实现算力资源的全局优化分配。

## 系统模块详细说明

### 用户交互层

#### 前端页面组件
- 功能概述
- 用户交互接口
- 与后端服务的交互机制
- 用户界面功能模块

### 应用管理层

#### 应用管理器

应用管理器是应用管理层的核心组件，负责应用的全生命周期管理，采用依赖注入设计模式，集成了工作空间服务、运行器服务、元数据服务、执行引擎平台和日志服务等多个子模块，为应用提供统一的生命周期管理接口。

**应用生命周期管理**：应用管理器实现了应用的创建、运行、停止和删除等全生命周期操作。在创建应用时，应用管理器首先创建应用元数据记录，将应用状态设置为克隆中，然后为应用创建执行引擎控制器实例，最后异步启动 Git 仓库克隆任务。克隆任务在后台 goroutine 中执行，使用 Git 命令行工具执行 `git clone` 操作，将应用代码从远程仓库下载到本地工作空间。克隆完成后，应用状态更新为未部署。当用户触发应用运行操作时，应用管理器更新应用状态为部署中，获取应用元数据和工作空间目录，创建运行器容器并启动应用主程序，应用主程序启动后更新应用状态为运行中。应用管理器采用状态机模式管理应用状态，包括克隆中、未部署、部署中、运行中、失败等状态，确保应用状态转换的正确性和一致性。

**工作空间管理**：工作空间服务为每个应用分配独立的工作目录，基于文件系统实现应用代码的存储和管理。工作空间服务使用 Git 命令行工具执行仓库克隆和更新操作，支持指定分支的单分支克隆，提高克隆效率。工作空间服务提供了完整的文件操作接口，包括获取文件树、读取文件内容、保存文件内容、创建文件、删除文件、创建目录和删除目录等功能。所有文件操作都包含路径安全检查，确保操作路径在工作空间目录内，防止路径遍历攻击。工作空间服务还支持测试仓库功能，当 Git 仓库 URL 为特殊标识符时，系统会从本地测试模板目录复制代码到工作空间，便于快速测试和开发。文件内容读取支持自动语言类型检测，根据文件扩展名识别编程语言类型，为代码编辑器提供语法高亮支持。

**运行器管理**：运行器服务基于 Docker 容器技术实现应用执行环境的隔离和管理。运行器服务使用 Docker Client API（Go 语言的 `github.com/moby/moby/client` 包）与 Docker 守护进程交互，执行容器的创建、启动、停止和删除操作。运行器服务支持多种编程语言运行时环境，通过配置不同的 Docker 镜像实现，如 Python 3.11、Node.js 等。创建运行器时，系统会为运行器容器配置环境变量，包括执行引擎地址、日志服务地址、环境安装命令和执行命令等，并将应用工作空间目录挂载到容器内的 `/iarnet/app` 路径，使得容器内可以访问应用代码。运行器容器启动后，容器内部会执行环境安装命令（如 `pip install`）来安装应用依赖，然后执行用户定义的应用启动命令，启动应用主程序。运行器服务采用领域驱动设计，将运行器封装为领域对象，通过管理器统一管理运行器实例的生命周期。

**应用元数据管理**：元数据服务采用内存缓存机制存储应用元数据，提供快速的元数据查询和更新能力。应用元数据包括应用 ID、名称、描述、Git 仓库地址、分支、运行环境类型、环境安装命令、执行命令、创建时间、更新时间和应用状态等信息。元数据服务实现了应用元数据的创建、查询、更新和删除等基本操作，支持应用状态的独立更新，便于应用生命周期管理。元数据服务使用线程安全的缓存实现，支持并发访问，通过应用 ID 作为键值快速定位应用元数据。虽然当前实现采用内存缓存，但通过服务接口抽象，可以轻松扩展为持久化存储实现，如数据库存储。

**日志服务**：日志服务负责收集和存储应用的运行日志，采用 SQLite 数据库作为持久化存储。日志服务支持日志的提交、查询和按时间范围检索等功能。日志服务通过 gRPC 接口接收运行器容器发送的日志条目，将日志信息存储到 SQLite 数据库中，支持按应用 ID、时间范围和日志级别等条件查询日志。日志服务为应用调试和问题排查提供了重要的可观测性支持，帮助用户了解应用的运行状态和错误信息。

#### 执行引擎

执行引擎基于课题二提出的核心技术实现，是应用管理层的核心执行组件，负责管理应用的执行逻辑、任务调度和 Actor 生命周期。执行引擎采用事件驱动架构和 Actor 模型，通过控制器管理器为每个应用创建独立的控制器实例，维护应用的函数定义、有向无环图（DAG）和 Actor 组信息，采用依赖驱动的调度策略根据 DAG 节点依赖关系触发任务执行。

**控制器管理**：执行引擎通过控制器管理器统一管理所有应用的控制器实例，每个应用对应一个控制器，控制器之间相互隔离。控制器管理器采用线程安全的映射结构存储控制器，支持控制器的创建、查询和会话管理。当应用主程序连接到执行引擎时，执行引擎首先从会话消息中获取应用 ID，然后查找对应的控制器实例，为控制器建立双向通信通道。控制器支持 WebSocket 和 gRPC 两种通信方式，通过 Protobuf 消息格式进行数据交换。控制器内部维护函数映射表、DAG 映射表和事件中心，函数映射表以函数名称为键存储函数定义，DAG 映射表以会话 ID 为键存储执行会话的 DAG 图，事件中心采用发布-订阅模式支持事件驱动的状态更新。

**函数定义与注册**：应用主程序通过发送 AppendPyFunc 消息向执行引擎注册函数定义，消息包含函数名称、参数列表、资源需求、副本数量、函数代码（Pickled 对象）和运行时语言等信息。执行引擎接收到函数注册请求后，首先根据资源需求通过组件服务部署指定数量的组件容器，每个组件容器对应一个 Actor 实例。组件部署成功后，执行引擎创建 Actor 组，将函数定义通过数据总线发送给每个 Actor，Actor 接收到函数定义后注册到本地运行时环境。Actor 组采用优先级队列管理 Actor 实例，优先级基于 Actor 的计算延迟和链路延迟计算，优先选择延迟较低的 Actor 执行任务，实现负载均衡和性能优化。函数注册完成后，执行引擎将函数定义存储到函数映射表中，函数定义包含函数名称、参数列表和 Actor 组信息。

**DAG 管理**：执行引擎为每个执行会话维护一个独立的 DAG 图，DAG 图包含控制节点和数据节点两种类型的节点。控制节点表示函数执行任务，包含函数名称、参数映射、前置数据节点依赖、输出数据节点和运行时 ID 等信息。数据节点表示数据对象，包含 Lambda ID、后续控制节点依赖、前置控制节点和对象引用等信息。DAG 图支持动态添加节点，当应用主程序发送 AppendDAGNode 消息时，执行引擎会根据节点类型创建对应的控制节点或数据节点，并自动构建节点之间的边关系。DAG 图维护挂起边列表，当节点尚未创建时，将边关系暂存到挂起列表中，待节点创建后自动完成边的构建。DAG 节点具有多种状态，包括等待执行（Pending）、就绪（Ready）、执行中（Running）、已完成（Done）和失败（Failed），执行引擎根据节点状态和依赖关系触发任务执行。

**依赖驱动调度策略**：执行引擎采用依赖驱动的调度策略，只有当节点的所有依赖都满足时，才会触发节点的执行。对于控制节点，执行引擎检查其前置数据节点是否都已就绪，如果所有前置数据节点都已完成，则标记控制节点为就绪状态。当控制节点就绪且所有参数都已添加时，执行引擎调用函数的 Invoke 方法触发任务执行。函数执行采用 Runtime 机制，每个函数调用对应一个 Runtime 实例，Runtime 维护函数参数、依赖集合和 Actor 引用。Runtime 使用条件变量（sync.Cond）实现依赖等待机制，当参数未全部就绪时，Runtime 会阻塞等待，直到所有依赖参数都添加完成。参数添加完成后，Runtime 从 Actor 组中选择一个 Actor，将函数调用请求发送给 Actor，Actor 执行计算任务并返回结果。执行引擎接收到 Actor 的执行响应后，更新控制节点状态为已完成，并将执行结果保存到输出数据节点，触发后续依赖该数据节点的控制节点执行。

**与算力执行层的交互**：执行引擎通过组件服务与算力执行层交互，当需要部署函数时，执行引擎分析函数的资源需求（CPU、内存、GPU 和资源标签），调用组件服务的 DeployComponent 方法请求部署组件容器。组件服务将部署请求转发给域资源管理器，域资源管理器在本地或跨节点范围内查找可用资源，确定目标资源提供者后调用部署接口创建组件容器。组件容器启动后，通过 gRPC 连接到执行引擎建立执行会话，接收函数定义消息并注册到本地运行时环境。执行引擎通过数据总线向 Actor 发送函数调用请求，Actor 从 Store 服务获取函数参数，执行计算任务，将结果保存到 Store 服务，并通过数据总线返回执行响应。Store 服务作为域资源管理器的重要组成部分，采用对象引用机制存储函数参数和结果，实现了数据的解耦和高效传输。执行引擎还通过事件中心发布 DAG 节点状态变更事件，支持前端实时监控任务执行状态。

### 算力执行层

#### 域资源管理器

域资源管理器是课题三提出的资源调度系统的核心组件，作为算力层提供给应用层的算力网络入口，集成了 Store 服务、组件管理、资源提供者管理、发现服务和调度服务等多个子模块，负责资源分配调度和实际计算资源的提供。系统设计目标是将一个 IARNet 节点视为一个域，域资源管理器首先在本地节点查找可用的资源提供者，如果本地资源不足则通过发现服务查找其他 IARNet 节点（对等节点），若仍无法满足资源需求，域资源管理器将调度请求委托给全局调度器进行跨节点调度。

**作为算力网络入口的作用**：域资源管理器为应用层提供了统一的资源访问接口，屏蔽了底层资源管理的复杂性。当执行引擎需要部署组件时，域资源管理器接收资源请求（包括 CPU、内存、GPU 和资源标签要求），通过组件服务查找可用的资源提供者，确定目标资源提供者后调用部署接口创建组件容器。域资源管理器维护节点 ID 和域信息，支持节点级别的资源管理和跨节点资源调度，实现了算力资源的统一管理和协同执行。

**Store 服务**：Store 服务作为域资源管理器的重要组成部分，采用对象引用机制存储函数参数和计算结果，实现了数据的解耦和高效传输。Store 服务支持对象存储和流式数据存储两种模式，对象存储用于存储完整的函数参数和结果，流式数据存储用于存储大型数据的分块传输。Store 服务采用内存存储实现，使用线程安全的映射结构存储对象，通过对象 ID 和源标识符（Source）唯一标识数据对象。Store 服务为 Actor 之间的数据共享和传递提供了统一的数据存储接口，Actor 通过对象引用获取参数和保存结果，避免了直接的数据传输，提高了系统的可扩展性和性能。

**组件管理**：组件管理服务负责组件的部署和生命周期管理。当域资源管理器接收到组件部署请求时，组件服务首先根据运行时环境（如 Python）选择对应的容器镜像，然后通过资源提供者服务查找满足资源要求的可用 Provider，确定目标资源提供者后调用部署接口创建组件容器。组件服务维护组件映射表，为每个组件分配唯一标识符，组件启动后通过数据总线连接到域资源管理器，建立双向通信通道。组件服务还负责组件的状态管理和消息路由，通过组件管理器统一管理所有组件的生命周期。

**资源提供者管理**：资源提供者管理服务负责资源提供者的注册、连接、查询和注销等操作。资源提供者管理服务支持从持久化存储（如数据库）加载已注册的 Provider，并在启动时自动建立连接。资源提供者管理服务实现了资源提供者的查找算法，优先使用缓存数据查找满足资源要求的 Provider，如果找不到合适的 Provider，则强制刷新资源容量后重试。资源提供者管理服务还支持资源标签匹配，检查 Provider 是否支持请求的资源类型（如 CPU、GPU、内存、摄像头等），确保资源调度的准确性。

**发现服务与 Gossip 协议**：发现服务基于 Gossip 协议实现多跳消息传播，在节点间交换资源状态信息。发现服务维护已知节点列表，定期执行 Gossip 操作，与已知的对等节点交换节点信息（包括节点 ID、地址、资源容量和资源标签等）。Gossip 消息包含 TTL（Time To Live）和最大跳数限制，支持多跳传播，使得节点能够发现间接连接的节点。发现服务还支持主动资源查询，当域资源管理器需要查找满足特定资源要求的节点时，发现服务可以在已知节点范围内进行资源匹配查询。发现服务通过 gRPC 接口与其他节点通信，采用异步消息传递机制，提高了资源发现的效率和可扩展性。

**跨节点调度机制**：当本地节点和域内对等节点都无法满足资源需求时，域资源管理器将调度请求委托给调度服务进行跨节点调度。调度服务首先通过发现服务查找满足资源要求的远程节点，然后通过 gRPC 连接到远程节点的调度服务，发送部署请求。远程节点接收到部署请求后，在本地执行组件部署，并将部署结果返回给请求节点。调度服务支持上游服务地址覆盖，当组件部署在远程节点时，可以指定上游的 ZMQ 地址、Store 地址和 Logger 地址，使得远程组件能够正确连接到上游服务。跨节点调度机制实现了算力资源的全局优化分配，支持大规模分布式计算场景。

#### 资源提供者与本地资源管理器

资源提供者节点是课题三提出的资源调度系统的重要组成部分，每个资源提供者节点封装或集成一个本地资源管理器。本地资源管理器在系统架构中直接采用 Docker、Kubernetes 等主流资源管理框架或异构资源作为其实现基础，在 IARNet 系统的业务场景下，这些资源管理框架被定位为本地资源管理器。需要说明的是，本地资源管理器并非本系统的实现范畴，而是系统直接利用的现有资源管理技术。系统通过统一的资源提供者接口对不同类型的本地资源管理器进行抽象，实现了对异构资源的广泛兼容性和可扩展性。不同资源提供者节点采用不同的本地资源管理器实现：Docker Provider 以 Docker 引擎作为其本地资源管理器，Kubernetes Provider 以 Kubernetes 集群作为其本地资源管理器，Process Provider 则采用进程管理作为其本地资源管理器。这种设计架构使得系统能够流畅适配当前主流的资源管理技术，通过统一的资源提供者接口对 IARNet 平台屏蔽了底层资源管理技术的实现差异，实现了良好的可扩展性，便于未来接入更多类型的资源管理平台。

**Docker Provider**：Docker Provider 以 Docker 引擎作为其本地资源管理器，通过 Docker Client API 与 Docker 守护进程交互。Docker Provider 支持容器的创建、启动、停止和删除操作，可以配置容器的资源限制（CPU、内存、GPU）、环境变量和网络设置。Docker Provider 通过健康检查机制定期查询容器的资源使用情况，更新资源容量缓存，确保资源调度的准确性。Docker Provider 还支持容器镜像管理，可以指定容器镜像和运行时环境，为组件提供隔离的执行环境。

**Kubernetes Provider**：Kubernetes Provider 以 Kubernetes 集群作为其本地资源管理器，通过 Kubernetes API 与集群交互。Kubernetes Provider 支持 Pod 的创建、调度和管理，可以利用 Kubernetes 的自动扩缩容、负载均衡和故障恢复等特性。Kubernetes Provider 通过 ConfigMap 和 Secret 管理配置信息和敏感数据，支持多命名空间部署，实现了更好的资源隔离和管理。Kubernetes Provider 还支持持久化存储和网络策略配置，为复杂应用场景提供了更强大的支持。

**Process Provider**：Process Provider 采用进程管理作为其本地资源管理器，直接在宿主机上启动和管理进程。Process Provider 适用于轻量级计算任务，避免了容器化的开销，提供了更高的执行效率。Process Provider 支持进程的资源限制和监控，可以配置进程的 CPU 亲和性和内存限制，确保资源使用的可控性。

**统一的资源提供者接口**：所有资源提供者节点都实现了统一的资源提供者接口，包括连接（Connect）、获取容量（GetCapacity）、获取可用资源（GetAvailable）、部署（Deploy）、健康检查（HealthCheck）和断开连接（Disconnect）等方法。统一的接口设计使得域资源管理器可以以相同的方式与不同类型的资源提供者交互，无需关心底层实现细节。资源提供者接口还定义了资源标签（ResourceTags）机制，用于描述 Provider 支持的计算资源类型（如 CPU、GPU、内存、摄像头等），支持基于标签的资源匹配和调度。

**异构资源兼容性**：系统通过资源提供者接口实现了对异构资源的统一抽象，不同类型的本地资源管理器（Docker 引擎、Kubernetes 集群、进程管理等）都可以通过实现统一的资源提供者接口接入系统资源池。系统不感知底层资源管理技术的具体实现细节，只需要资源提供者节点能够提供统一的资源管理能力（资源交互、组件生命周期管理、资源容量管理等），这些能力都可以利用本地资源管理器的原生方式实现。由于本地资源管理器并非系统实现范畴，系统无需关心其内部机制，只需要通过资源提供者接口调用其提供的资源管理能力即可。这种设计使得系统能够同时支持多种异构资源，用户可以根据实际需求选择最适合的资源管理技术，实现了资源的灵活配置和高效利用。

**可扩展性设计**：系统的可扩展性体现在两个方面：一是可以轻松接入新的资源管理技术，只需要实现统一的资源提供者接口即可；二是可以同时使用多种资源管理技术，不同类型的资源提供者节点可以在同一个 IARNet 节点中共存，系统会根据资源需求和资源标签自动选择合适的资源提供者。资源提供者接口定义了标准的资源管理操作（连接、获取容量、部署、健康检查等），任何实现了这些接口的资源管理技术都可以接入系统，无需修改系统核心代码。这种可扩展性设计使得系统能够适应不断发展的资源管理技术，保持系统的先进性和竞争力。

**资源交互与生命周期管理**：资源提供者节点通过调用本地资源管理器的原生 API 实现资源交互和组件生命周期管理，系统不感知底层实现细节。资源交互（如资源查询、容量获取等）和组件生命周期管理（创建、启动、停止、删除等）都利用本地资源管理器的原生方式实现，资源提供者节点只是作为适配层，将系统的资源请求转换为本地资源管理器的 API 调用。由于本地资源管理器提供了成熟的资源管理能力，系统无需重复实现这些功能，只需通过资源提供者接口调用即可。这种设计使得系统能够充分利用本地资源管理器的成熟功能和性能优化，同时保持系统的简洁性和可维护性。资源容量管理同样利用本地资源管理器的原生能力，资源提供者节点通过查询本地资源管理器的资源使用情况获取容量信息，系统无需实现复杂的资源统计和监控逻辑。

#### 组件与 Actor

组件是系统的核心执行单元，每个组件封装一个 Actor，Actor 是实际执行计算任务的实体，采用消息驱动的异步处理模式。组件通过 Docker 容器运行，提供了隔离的执行环境，支持多种编程语言运行时（如 Python、Node.js 等）。

**组件功能概述**：组件是资源调度的基本单位，当执行引擎需要部署函数时，会根据函数的资源需求和副本数量部署相应数量的组件容器。组件容器启动后，通过 gRPC 连接到执行引擎建立执行会话，接收函数定义消息并注册到本地运行时环境，注册完成后通过数据总线向上游发送就绪消息。组件管理器维护所有组件的映射表，为每个组件分配唯一标识符，通过数据总线实现组件与执行引擎之间的双向通信。

**Actor 执行模型**：Actor 是组件内部的核心执行单元，采用 Actor 模型的并发处理模式。每个 Actor 维护自己的状态和消息队列，通过消息传递与其他 Actor 和执行引擎通信。Actor 采用异步处理模式，接收到函数调用请求后，从 Store 服务获取参数，执行计算任务，将结果保存到 Store 服务，并通过数据总线返回执行响应。Actor 还维护延迟信息（计算延迟和链路延迟），用于负载均衡和性能优化。

**消息驱动的异步处理**：Actor 采用消息驱动的异步处理模式，通过数据总线接收和发送消息。Actor 启动后，首先等待接收函数定义消息，注册函数到本地运行时环境。注册完成后，Actor 进入就绪状态，等待函数调用请求。当接收到函数调用请求时，Actor 从 Store 服务获取参数，执行函数计算，将结果保存到 Store 服务，并通过数据总线返回执行响应。消息驱动的异步处理模式使得 Actor 能够高效地处理并发请求，提高了系统的吞吐量和响应速度。

**函数定义接收与注册**：组件启动后，Actor 通过 gRPC 连接到执行引擎，建立执行会话。执行引擎发送函数定义消息给 Actor，消息包含函数名称、参数列表、依赖包列表、函数代码（Pickled 对象）和运行时语言等信息。Actor 接收到函数定义消息后，首先安装依赖包（如果有），然后反序列化函数对象，注册函数到本地运行时环境。函数注册成功后，Actor 发送确认消息给执行引擎，标识自身已就绪，可以接收函数调用请求。

**任务执行流程**：当执行引擎需要执行函数时，通过数据总线向 Actor 发送函数调用请求，请求包含运行时 ID、参数列表（对象引用）等信息。Actor 接收到函数调用请求后，首先从 Store 服务获取参数对象，反序列化参数数据，然后调用注册的函数执行计算任务。函数执行过程中，Actor 记录计算开始时间和结束时间，计算执行延迟。函数执行完成后，Actor 将结果序列化并保存到 Store 服务，获取结果的对象引用，通过数据总线返回执行响应给执行引擎。执行响应包含运行时 ID、结果对象引用和执行信息（包括计算延迟）等，执行引擎接收到响应后更新 DAG 节点状态，触发后续任务的执行。

#### 数据总线

数据总线是系统内组件间通信的核心基础设施，采用 Router-Dealer 模式的 ZMQ 通道实现消息路由，执行引擎通过数据总线向 Actor 发送函数调用请求，Actor 通过数据总线返回执行响应。

**功能概述**：数据总线提供了组件与执行引擎之间的双向通信通道，支持异步消息传递和消息路由。数据总线采用 ZMQ（ZeroMQ）高性能消息库实现，提供了低延迟、高吞吐量的消息传递能力。数据总线还支持消息队列机制，当组件尚未连接时，消息会被暂存到队列中，待组件连接后自动发送，确保消息的可靠传递。

**ZMQ 通信机制**：ZMQ 是一个高性能的异步消息库，提供了多种消息传递模式。数据总线采用 Router-Dealer 模式，Router 端（执行引擎）可以同时与多个 Dealer 端（组件）通信，实现了多对多的消息传递。ZMQ 支持 TCP 和 IPC 两种传输方式，TCP 用于跨网络通信，IPC 用于本地进程间通信。ZMQ 还支持消息的自动重连和故障恢复，提高了通信的可靠性。

**Router-Dealer 模式**：Router-Dealer 模式是 ZMQ 提供的一种消息传递模式，Router 端负责消息的路由和分发，Dealer 端负责消息的接收和发送。在数据总线中，执行引擎作为 Router 端，维护所有组件的连接信息，根据组件 ID 将消息路由到对应的组件。组件作为 Dealer 端，连接到 Router 端，接收和发送消息。Router-Dealer 模式支持异步消息传递，Router 端可以同时向多个 Dealer 端发送消息，Dealer 端可以异步接收和发送消息，实现了高效的并发通信。

**消息路由机制**：数据总线通过组件 ID 实现消息路由，执行引擎根据组件 ID 将消息路由到对应的组件。数据总线维护组件连接状态，当组件连接时，标记组件为已连接状态，并发送所有待发送的消息。当组件断开连接时，标记组件为未连接状态，后续消息会被暂存到队列中。消息路由机制还支持消息的优先级和超时处理，确保重要消息能够及时传递。

**组件间通信流程**：组件启动后，通过 ZMQ Dealer Socket 连接到数据总线的 Router 端，建立通信连接。连接建立后，组件发送就绪消息，标识自身已就绪，可以接收消息。执行引擎接收到就绪消息后，标记组件为已连接状态，并发送所有待发送的消息。当执行引擎需要向组件发送消息时，通过 Router Socket 将消息发送到对应的组件，消息包含组件 ID 和消息内容。组件接收到消息后，解析消息内容，执行相应的处理逻辑，并通过 Dealer Socket 将响应消息发送回执行引擎。数据总线还支持消息的批量发送和接收，提高了通信效率。

#### Store 服务

Store 服务作为域资源管理器的重要组成部分，采用对象引用机制存储函数参数和计算结果，实现了数据的解耦和高效传输，为 Actor 之间的数据共享和传递提供了统一的数据存储接口。

**功能概述**：Store 服务提供了对象存储和流式数据存储两种模式，对象存储用于存储完整的函数参数和结果，流式数据存储用于存储大型数据的分块传输。Store 服务采用内存存储实现，使用线程安全的映射结构存储对象，通过对象 ID 和源标识符（Source）唯一标识数据对象。Store 服务还支持对象引用机制，Actor 通过对象引用获取参数和保存结果，避免了直接的数据传输，提高了系统的可扩展性和性能。

**对象引用机制**：对象引用机制是 Store 服务的核心设计，每个数据对象都有一个唯一的对象引用（ObjectRef），包含对象 ID 和源标识符。对象引用机制实现了数据的解耦，Actor 之间通过对象引用传递数据，而不是直接传递数据内容，减少了网络传输的开销。对象引用机制还支持跨节点数据访问，当组件部署在远程节点时，可以通过对象引用访问上游节点的数据，实现了数据的分布式存储和访问。

**数据存储与检索**：Store 服务提供了 SaveObject 和 GetObject 方法用于对象的存储和检索。SaveObject 方法接收编码后的对象数据，生成唯一的对象 ID，将对象存储到内存映射中，并返回对象引用。GetObject 方法根据对象引用获取对象数据，如果对象不存在则返回错误。Store 服务还支持流式数据存储，通过 SaveStreamChunk 和 GetStreamChunk 方法实现大型数据的分块存储和检索。流式数据存储使用条件变量实现阻塞等待机制，当请求的数据块尚未到达时，请求会阻塞等待，直到数据块到达后返回。

**数据解耦设计**：数据解耦设计是 Store 服务的重要特性，通过对象引用机制实现了数据与执行逻辑的解耦。Actor 之间通过对象引用传递数据，而不是直接传递数据内容，使得数据可以在不同的 Actor 之间共享和传递，提高了系统的灵活性和可扩展性。数据解耦设计还支持数据的延迟加载，Actor 可以在需要时才从 Store 服务获取数据，减少了内存占用和网络传输开销。

**为 Actor 提供的数据共享接口**：Store 服务为 Actor 提供了统一的数据共享接口，Actor 可以通过 gRPC 接口访问 Store 服务，存储和检索数据对象。Store 服务客户端封装了与 Store 服务的通信逻辑，提供了简单易用的 API，Actor 只需要调用相应的方法即可完成数据的存储和检索。Store 服务还支持数据的序列化和反序列化，自动处理不同数据类型的编码和解码，简化了 Actor 的使用。Store 服务的数据共享接口还支持数据的批量操作和异步操作，提高了数据访问的效率。
