# 3.3.4 实验设计

为了全面评估分级资源调度方法在广域分布式算力网络环境中的有效性和性能表现，本研究设计并实施了一系列系统性实验。实验旨在验证跨域资源调度框架在不同网络规模下的调度效率、资源利用率、系统吞吐量以及任务执行成功率等关键指标。

## 3.3.4.1 实验环境

**实验环境**：
- Python Version: 3.12.3
- Go Version: 1.25.5
- Docker Version: 29.1.4
- CPU: AMD EPYC 7C13 64-Core Processor
- Memory: 512GB
- Operating System: Ubuntu 24.04.3 LTS x86_64


实验环境采用Docker容器化技术构建分布式算力网络测试平台，通过Docker Compose编排管理多个iarnet资源管理节点和资源提供者实例。每个iarnet资源管理节点运行在独立的Docker容器中，配置固定的IP地址（172.30.0.10-172.30.0.17），通过Docker桥接网络实现节点间的网络通信。节点间采用Gossip协议进行资源状态同步，Gossip更新间隔配置为1-2秒的随机区间。资源提供者采用Mock Provider实现，每个资源提供者配置统一的资源容量：16核CPU（16000毫核）、32GB内存、2个GPU设备。实验代码运行在独立的实验容器中，通过Docker Socket挂载实现与Docker守护进程的交互，能够直接调用资源管理节点的调度接口，模拟真实执行引擎的任务提交行为。

## 3.3.4.2 工作负载设计

实验工作负载设计采用混合任务类型模型，任务类型分为三类：小任务（Small）、中任务（Medium）和大任务（Large）。小任务CPU需求为300-700毫核，内存需求为512MB-1GB，不占用GPU资源，执行时间范围为500-2000毫秒，在总任务数中占比20%。中任务CPU需求为1500-2500毫核，内存需求为2GB-3GB，不占用GPU资源，执行时间范围为2000-5000毫秒，在总任务数中占比70%。大任务CPU需求为3500-4500毫核，内存需求为4GB-6GB，需要占用1个GPU设备，执行时间范围为5000-10000毫秒，在总任务数中占比10%。任务资源需求在各自类型的规定区间内随机分配，任务类型序列在实验开始前预先生成并随机打乱，确保小、中、大任务混合提交。

实验总任务数设置为2000个，任务提交速率固定为20请求/秒（req/s）。每个任务最多允许3次重试，重试延迟在5-15秒范围内随机分配。

## 3.3.4.3 实验场景配置

实验设计了四种不同规模的网络场景，分别对应1节点、2节点、4节点和8节点的算力网络拓扑。单节点场景（1节点）作为基线场景，所有任务均在本地节点执行，不存在跨域调度。双节点场景（2节点）引入跨域调度能力，两个节点通过Gossip协议进行资源状态同步。四节点场景（4节点）和八节点场景（8节点）进一步扩大网络规模，评估系统在中等和大规模分布式环境下的性能表现。所有场景下的实验配置保持一致，包括任务类型比例、提交速率、资源提供者配置等，确保不同场景间实验结果的可比性。

## 3.3.4.4 评估指标

实验设计了全面的性能评估指标体系，涵盖任务调度成功率、系统响应时延、系统吞吐量、资源利用率、本地与跨节点执行比例等关键指标。

任务调度成功率定义为成功完成调度的任务数占总任务数的百分比。响应时延（Response Latency）定义为从任务提交到成功完成调度部署的时间间隔，实验统计响应时延的平均值、P50（中位数）、P95和P99分位数。系统吞吐量（Throughput）定义为单位时间内成功完成的任务数，单位为请求/秒（req/s）。资源利用率指标包括CPU利用率、内存利用率和GPU利用率，实验在每个节点上定期采集资源使用情况，每完成100个任务采集一次。本地执行比例和跨节点执行比例反映调度系统的负载分布特征，实验还按任务类型统计各类型任务在本地和跨节点的执行分布。

## 3.3.4.5 实验结果

### 3.3.4.5.1 任务调度成功率

如图1所示，我们测试了不同网络规模场景（1节点、2节点、4节点、8节点）下的任务调度成功率。实验结果显示，随着网络规模的增加，系统整体成功率呈现上升趋势。在1节点场景下，由于资源容量有限，部分任务因资源不足而失败，成功率为XX%。随着节点数量增加到2节点、4节点和8节点，系统总资源容量成倍增长，能够处理更大规模的工作负载，成功率分别提升至XX%、XX%和XX%。这表明跨域资源调度机制能够有效利用分布式网络中的资源，提高系统的资源供给能力和任务调度成功率。

![任务调度成功率](experiment/result/fig1_task_success_rate_en.png)

进一步分析各类型任务的成功率分布，如图1(b)所示，小任务（Small）在所有场景下均保持较高的成功率（XX%-XX%），这主要因为小任务的资源需求较低，容易在本地或远程节点找到可用资源。中任务（Medium）的成功率随网络规模增加而显著提升，从1节点场景的XX%提升至8节点场景的XX%，说明跨域调度对中等规模任务具有明显的优化效果。大任务（Large）由于需要GPU资源且资源需求较高，在1节点场景下成功率相对较低（XX%），但随着节点数量增加，成功率提升至XX%，表明跨域调度能够有效解决GPU资源稀缺的问题。

### 3.3.4.5.2 任务执行分布

如图2所示，我们分析了不同场景下任务的执行分布情况。图2(a)展示了总体执行分布，可以看出随着网络规模增加，跨节点执行的比例逐渐上升。在1节点场景下，所有任务均在本地执行，跨节点执行比例为0%。在2节点场景下，跨节点执行比例达到XX%，表明当本地资源不足时，系统能够有效将任务调度到远程节点。在4节点和8节点场景下，跨节点执行比例分别达到XX%和XX%，说明系统能够充分利用分布式网络中的资源，实现负载均衡。

![任务执行分布](experiment/result/fig2_execution_distribution_en.png)

图2(b)展示了按任务类型的执行分布。可以看出，小任务主要倾向于本地执行，跨节点执行比例较低（XX%-XX%），这符合本地优先调度策略的设计目标。中任务和大任务的跨节点执行比例相对较高，分别达到XX%-XX%和XX%-XX%，说明当任务资源需求较高时，系统会主动利用跨域资源来满足调度需求。

图2(c-f)展示了各场景下各节点的任务执行分布。可以看出，在2节点、4节点和8节点场景下，任务能够相对均匀地分布到各个节点，体现了跨域调度机制的负载均衡能力。特别是在8节点场景下，所有8个节点都参与了任务执行，其中node.5执行了XX个任务（XX%），虽然数量相对较少，但证明了系统能够有效利用所有可用节点资源。

### 3.3.4.5.3 响应时延与系统吞吐量

如图3所示，我们测试了不同场景下的响应时延和系统吞吐量。响应时延统计结果显示，随着网络规模增加，平均响应时延呈现轻微上升趋势，从1节点场景的XX毫秒增加到8节点场景的XX毫秒，这主要因为跨节点调度需要额外的网络通信开销。然而，P50、P95和P99分位数显示，时延分布相对稳定，P95时延保持在XX-XX毫秒范围内，P99时延保持在XX-XX毫秒范围内，表明系统在大多数情况下能够保持较低的调度时延。

![响应时延与系统吞吐量](experiment/result/fig3_latency_throughput_en.png)

系统吞吐量方面，随着网络规模增加，系统吞吐量显著提升。在1节点场景下，系统吞吐量为XX req/s。随着节点数量增加到2节点、4节点和8节点，系统吞吐量分别提升至XX req/s、XX req/s和XX req/s，接近线性增长趋势。这表明系统具有良好的可扩展性，能够通过增加节点数量来提升整体处理能力。

### 3.3.4.5.4 资源利用率

如图4所示，我们分析了不同场景下的资源利用率。可以看出，随着网络规模增加，各节点的平均资源利用率呈现下降趋势，这主要因为总资源容量成倍增长，而工作负载保持不变。在1节点场景下，CPU、内存和GPU的平均利用率分别为XX%、XX%和XX%。在8节点场景下，平均利用率分别降至XX%、XX%和XX%，表明系统具有充足的资源余量，能够支持更高强度的工作负载。

![资源利用率](experiment/result/fig4_resource_utilization_en.png)

进一步分析各节点的资源利用率分布，可以看出在2节点、4节点和8节点场景下，各节点的资源利用率相对均衡，CPU利用率差异在XX%以内，内存利用率差异在XX%以内，表明跨域调度机制能够有效实现负载均衡，避免某些节点过载而其他节点资源闲置的情况。
