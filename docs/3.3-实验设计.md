## 3.3 面向算力网络的分级资源调度方法：实验设计

本节给出一套围绕 3.3 章整体技术（资源态势感知、跨域分级调度、调度委托）的**端到端总体实验方案**，侧重体现：

- 随 `iarnet` 节点数增加，系统调度**吞吐量近似线性提升**；
- 分级调度框架使**入口节点本地资源压力降低**、其他节点资源得到有效利用；
- 在引入跨节点网络开销的情况下，请求**响应时延仍处于可接受范围**；
- 在统一的任务混合负载（小/中/大任务比例固定）下，系统能够自动完成本地与跨域调度的协同。  

为便于论文中绘图与复现实验，以下仅设计 **一个主实验**，通过改变 `iarnet` 节点数 \(N\) 与网络开销等参数，即可得到多组可对比数据。

---

### 3.3.1 实验环境与基本配置

#### 3.3.1.1 硬件与软件环境

- **物理/虚拟机数量**：不少于 5 台（或 1 台大机器上起 5 个独立 `iarnet` 实例），每台配置：
  - CPU：4–8 核
  - 内存：16–32 GB
- **操作系统**：Ubuntu 22.04 LTS
- **容器环境**：Docker 24.0+
- **运行时**：Go 1.21+，IARNet 最新版本
- **网络环境**：节点间 RTT 约 5–20 ms，可通过 `tc/netem` 或在 RPC 层插入 `sleep` 模拟固定网络时延。

#### 3.3.1.2 拓扑与节点角色

- **节点数 \(N\)**：\(N \in \{1, 2, 3, 4, 5\}\)。
- **每个 `iarnet` 节点**：
  - 管理 **3 个 Provider**，所有 Provider 配置完全相同；
  - 每个 Provider 资源容量：CPU 8000 mC、内存 16 GB、GPU 1；
  - 所有节点的总资源规模与 Provider 数量随 \(N\) 线性增长，保证“单位节点形态一致”。
- **入口节点**：统一从 Node-1 提交所有调度请求，由分级调度框架自动决定是在本地还是委托到其他节点执行。

#### 3.3.1.3 任务工作负载与时延模型

- **任务类型与比例**（按任务“个数”严格控制比例）：
  - 小任务（30%）：CPU 500 mC，Memory 256 MB，GPU 0；
  - 中任务（50%）：CPU 2000 mC，Memory 2 GB，GPU 0；
  - 大任务（20%）：CPU 4000 mC，Memory 4 GB，GPU 1。
- **任务提交速率**：固定为 \(R\)（如 50 req/s），任务到达间隔可设为 \(1/R\) 的定间隔或带轻微抖动的泊松/高斯分布；
  - 总任务数 \(T\)：例如 10 000 个，使三类任务分别为 3000/5000/2000 个。
- **任务执行时延（模拟计算时间）**：在 Provider 内以 `sleep` 或定时器模拟：
  - 小任务：在 \([50, 200]\) ms 区间内均匀随机；
  - 中任务：在 \([200, 800]\) ms 区间内均匀随机；
  - 大任务：在 \([800, 2000]\) ms 区间内均匀随机；
  - 任务完成时，自动释放其占用的资源（CPU/内存/GPU）以便后续任务复用。
- **请求超时控制**：
  - 为每个调度请求设置统一超时时间 \(T_{\text{timeout}}\)（例如 3 s 或 5 s，可根据执行时延上界和网络时延综合设定）；
  - 若在 \(T_{\text{timeout}}\) 内仍未完成部署（未拿到 Provider 并开始执行），则视为**调度失败请求**，客户端立刻返回失败；即便任务后来在系统内部被处理完成，也不再计入成功请求统计。
- **网络开销模型**：
  - 本地 Provider 调用：仅本机 gRPC 和容器启动开销，可近似忽略或设为 1–2 ms；
  - 跨节点访问：在远程 `scheduler` 或 Provider 的 RPC 调用中注入固定延迟（如 5/10/20 ms），通过 `time.Sleep` 或 `netem` 实现。

#### 3.3.1.4 调度策略与系统配置

整个实验始终使用项目实现的 **分级调度框架**：

- 本地优先：若入口节点本地资源满足请求，则直接在本地 Provider 部署；
- 资源不足：当本地返回“failed to find available provider”等错误时，自动触发跨域调度；
- 跨域调度：通过 Gossip 发现其它在线节点，结合节点资源视图选择合适的远程节点，由远程节点完成最终部署；
- 调度委托：跨域调度时采用两阶段提交（Propose/Commit），可在配置中启用资源安全裕度策略等。

> 基线与对比全部基于同一调度实现，仅改变 **参与调度的 `iarnet` 节点数量 \(N\)**，从而突出“节点规模扩展”本身带来的性能与资源利用差异。

---

### 3.3.2 实验：节点规模扩展下的分级调度性能评估

#### 3.3.2.1 实验目标

在保持单节点形态、单 Provider 容量，以及任务到达速率和任务混合比例**完全相同**的前提下：

1. 以 \(N=1\) 的单节点场景作为**基线**，测量系统在该资源规模下的吞吐量、响应时延和资源利用情况；  
2. 随着节点数 \(N\) 增加到 2、3、4、5，观察：  
   - 系统整体吞吐量是否随总资源线性提升；  
   - 入口节点本地资源利用是否降低，其它节点资源是否被有效激活；  
   - 引入跨节点网络时延后，请求响应时延的增加是否在可接受范围内。

#### 3.3.2.2 实验步骤

1. **基线实验（N = 1）**  
   1. 启动 1 个 `iarnet` 节点（Node-1），管理 3 个 Provider；  
   2. 启动任务发生器，以固定速率 \(R\) 提交共 \(T\) 个调度请求，任务类型严格按 30%/50%/20% 生成；  
   3. 对每个请求记录：  
      - `t_submit`：任务提交时间戳；  
      - `t_dispatch_start`：调度服务开始处理的时间；  
      - `t_deploy`：Provider 完成部署、开始执行的时间；  
      - `t_finish`：模拟执行完成并释放资源的时间；  
      - `node_id` / `provider_id`：实际执行的节点和 Provider；  
      - `task_type`：小/中/大；  
      - `is_cross_node=false`（单节点场景恒为 false）。  
   4. 按“提交任务数量”分批统计入口节点下**托管资源**的使用情况：设定批大小 \(B\)（如 100 个任务），当累计提交任务数达到 \(B, 2B, 3B, ...\) 时，对当前仍在运行的任务进行一次逻辑资源统计：  
      - 对 Node-1：基于其管理的 3 个 Provider 的容量模型，统计 `Capacity.Used.Total`（CPU/内存/GPU）之和占 `Capacity.Total` 之比，得到该节点**托管资源的逻辑利用率**；  
      - 对每个 Provider：同样使用其 `Capacity.Total/Used/Available` 计算被占用资源比例。  
      注意：这里统计的是 `iarnet` 节点下**所管理资源池**的使用情况，而非宿主机进程或物理设备的系统级利用率。

2. **扩展实验（N = 2, 4, 8, 16）**
   对于每一个节点数量 \(N\)：  
   1. 启动 N 个 `iarnet` 节点（Node-1 ~ Node-N），每个节点均管理 3 个相同配置的 Provider；  
   2. 启用 Gossip 资源发现，等待一段时间（如 60 s）直至所有节点都出现在资源视图中；  
   3. 所有调度请求仍全部从 Node-1 提交，其余节点只作为被调度的执行节点；  
   4. 使用与基线**完全相同**的任务生成配置（提交速率 \(R\)、总任务数 \(T\)、任务比例与执行时延分布）；  
   5. 对每个任务记录与基线相同的字段，额外增加：  
      - `is_cross_node`：布尔值，标记是否选择了除 Node-1 以外的节点执行；  
   6. 同样按“提交任务数量”分批统计所有节点下托管资源池的逻辑利用率：在每个批次 \(B, 2B, 3B, ...\) 时，汇总各节点所有 Provider 的 `Capacity.Used`，计算节点级与全局级的资源利用率，作为对应横轴位置的数据点。  

3. **数据对齐与重复实验**  
   - 对每个 \(N\) 至少重复运行 3 轮实验，使用不同的随机种子产生任务到达时间和任务持续时间；  
   - 后续在分析时对同一 \(N\) 的多轮实验结果取平均值，并计算标准差或置信区间。  

#### 3.3.2.3 统计指标与计算方法

1. **按任务编号统计（横坐标为“已提交任务数”）**  
   - 将所有任务按 `t_submit` 排序，赋予顺序编号 \(k = 1, 2, ..., T\)；  
   - 以任务编号 \(k\) 作为横坐标，可绘制：  
     - 累积平均响应时延曲线：\(\overline{L_{\text{resp}}}(k)\)；  
     - 累积平均完成时延曲线：\(\overline{L_{\text{full}}}(k)\)；  
     - 累积吞吐量曲线：\(\text{Throughput}(k) = k_{\text{success}} / (t_{\text{last}} - t_{\text{submit},1})\)，其中 \(k_{\text{success}}\) 为截至编号 \(k\) 时成功请求个数，\(t_{\text{last}}\) 为当前最后一个完成/超时的时间戳；  
     - 累积失败率曲线：\(\text{FailRate}(k) = k_{\text{fail}} / k\)，其中 \(k_{\text{fail}}\) 为截至编号 \(k\) 的失败请求个数（含超时）。  

2. **单任务级指标**  
   - 响应时延：\(L_{\text{resp}} = t_{\text{deploy}} - t_{\text{submit}}\)；  
   - 完成时延：\(L_{\text{full}} = t_{\text{finish}} - t_{\text{submit}}\)；  
   - 是否跨节点：`is_cross_node`；  
   - 任务类型：`task_type`；  
   - 是否超时：`is_timeout`（若 `t_deploy - t_submit > T_{\text{timeout}}` 或在超时时间内未能获取调度结果，则置为 true）；  
   - 请求最终状态：`status ∈ {success, timeout, other_fail}`。  

3. **聚合指标**  
   对每个 \(N\) 计算：  
   - 平均响应时延 / 完成时延，以及 P50/P95/P99；  
   - 系统吞吐量：\(T / (t_{\text{finish,max}} - t_{\text{submit,min}})\)；  
   - 各节点平均 CPU/内存/GPU 利用率及其方差；  
   - 入口节点（Node-1）本地执行比例：`count(is_cross_node=false && status=success)/T`；  
   - 跨节点执行比例：`count(is_cross_node=true && status=success)/T`，以及不同远程节点分配的成功任务数量；  
   - 失败率：`FailRate = count(status≠success)/T`，其中可细分为超时失败率 `TimeoutRate` 与其它错误导致的失败率。  

#### 3.3.2.4 预期图表设计

1. **图1：吞吐量与时延随节点数变化（柱状图 + 折线图）**  
   - X 轴：节点数 \(N\)（1,2,3,4,5）；  
   - 左 Y 轴：系统吞吐量（req/s）；  
   - 右 Y 轴：平均响应时延或 P95 响应时延（ms）；  
   - 展示：随 \(N\) 增大吞吐量近似线性增长，而响应时延仅小幅上升。  

2. **图2：各节点托管资源利用率对比（堆叠柱状图或热力图）**  
   - 方案A：针对每个 \(N\)，绘制所有节点平均 CPU 利用率的柱状图：  
     - X 轴：节点编号（Node-1 ... Node-N）；  
     - Y 轴：平均 CPU 利用率%；  
     - 重点对比 N=1 与 N>1 场景：随着节点数增加，其它节点被更多地分配任务，其托管资源利用率显著上升，全网利用更均匀；入口节点是否削峰不是结论重点。  
   - 方案B：绘制“节点 × 时间”的资源利用率热力图，展示随着任务推进，各节点被逐步激活。  

3. **图3：本地 vs 跨节点调度比例随节点数变化（堆叠柱状图）**  
   - X 轴：节点数 \(N\)；  
   - Y 轴：任务比例（%）；  
   - 堆叠部分：入口节点本地执行比例 / 跨节点执行比例；  
   - 展示：随着 \(N\) 增大，跨节点执行比例提升，整体资源利用被摊开。  

4. **图4：响应时延分布（箱线图，按任务类型分组）**  
   - 对每个 \(N\)，分别对小/中/大任务绘制响应时延箱线图；  
   - 展示各任务类型在引入跨节点网络时延后的延迟分布，验证其仍在业务可接受范围内。  

5. **图5：任务编号 vs 累积平均响应时延（折线图）**  
   - X 轴：任务编号 \(k\)（或等价的“已提交任务数”）；  
   - Y 轴：\(\overline{L_{\text{resp}}}(k)\)；  
   - 多条线：不同 \(N\) 的累积平均响应时延曲线；  
   - 可叠加一条“累积失败率曲线”\(\text{FailRate}(k)\)，展示在不同 \(N\) 下随着任务数增加系统的稳定性；  
   - 展示：随着系统逐步进入稳态，累积平均时延稳定在一个可接受水平，同时失败率保持在较低水平，不随任务数量线性恶化。  

#### 3.3.2.5 实验结论预期（可写入论文）

- **资源利用角度**：  
  - 与单节点基线相比，当 \(N>1\) 时，入口节点资源利用峰值下降，其余节点资源被有效激活，全网资源利用率提高；  
  - 分级调度框架实现了“入口节点压力削峰 + 其它节点托底”的效果。  

- **吞吐量与扩展性角度**：  
  - 在任务到达速率和任务规模分布固定的前提下，系统总体吞吐量随 \(N\) 增大近似线性提升；  
  - 证明在统一调度逻辑与同构节点前提下，系统可以通过简单地横向增加 `iarnet` 节点来扩展可用算力。  

- **时延角度**：  
  - 引入跨节点网络开销后，跨域调度任务的响应时延略有上升，但整体平均响应时延仍保持在业务可接受范围内（可在论文中给出具体数值，如增加 <20%）；  
  - 本地优先策略保证大量小/中任务仍在入口节点快速完成，从而控制总体延迟。  

---

> 上述实验方案覆盖了 3.3 章中三类技术的协同效果：Gossip 资源视图支撑跨节点调度决策，分级调度框架实现本地与跨域协同，委托调度机制保证跨节点部署的可靠性。通过一套随节点数扩展的统一实验，可以在论文中给出简洁而有说服力的性能曲线与资源利用图。  


